{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "\n",
    "In the grid search approach, you literally experiment with all possible \n",
    "combinations for a defined set of values of a hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras doesnâ€™t directly provide the means to perform grid search tuning \n",
    "on the models. We can however use a custom for loop with the defined \n",
    "values for training or alternatively use the sklearn wrapper provided by \n",
    "Keras to package the model in an sklearn type object and then leverage the grid search method in sklearn to accomplish the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "#Generate dummy data for 3 features and 1000 samples\n",
    "x_train = np.random.random((1000, 3))\n",
    "#Generate dummy results for 1000 samples: 1 or 0\n",
    "y_train = np.random.randint(2, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a python function that returns a compiled DNN model\n",
    "def create_dnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=3, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabon\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kabon\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\kabon\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/60\n",
      "1000/1000 [==============================] - 0s 220us/step - loss: 0.7084 - acc: 0.4970\n",
      "Epoch 2/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.7056 - acc: 0.4900\n",
      "Epoch 3/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.7031 - acc: 0.4830\n",
      "Epoch 4/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.7012 - acc: 0.4840\n",
      "Epoch 5/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6998 - acc: 0.4990\n",
      "Epoch 6/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6989 - acc: 0.5070\n",
      "Epoch 7/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6981 - acc: 0.5090\n",
      "Epoch 8/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6976 - acc: 0.5040\n",
      "Epoch 9/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6969 - acc: 0.5010\n",
      "Epoch 10/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6965 - acc: 0.5030\n",
      "Epoch 11/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6960 - acc: 0.5030\n",
      "Epoch 12/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6957 - acc: 0.5080\n",
      "Epoch 13/60\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6954 - acc: 0.5110\n",
      "Epoch 14/60\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.6950 - acc: 0.5090\n",
      "Epoch 15/60\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 0.6948 - acc: 0.5100\n",
      "Epoch 16/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6945 - acc: 0.5140\n",
      "Epoch 17/60\n",
      "1000/1000 [==============================] - 0s 20us/step - loss: 0.6942 - acc: 0.5170\n",
      "Epoch 18/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6939 - acc: 0.5130\n",
      "Epoch 19/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6936 - acc: 0.5120\n",
      "Epoch 20/60\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 0.6934 - acc: 0.5180\n",
      "Epoch 21/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6932 - acc: 0.5160\n",
      "Epoch 22/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6930 - acc: 0.5120\n",
      "Epoch 23/60\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 0.6928 - acc: 0.5100\n",
      "Epoch 24/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6927 - acc: 0.5120\n",
      "Epoch 25/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6925 - acc: 0.5190\n",
      "Epoch 26/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6924 - acc: 0.5190\n",
      "Epoch 27/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6922 - acc: 0.5140\n",
      "Epoch 28/60\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.6922 - acc: 0.5160\n",
      "Epoch 29/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6920 - acc: 0.5150\n",
      "Epoch 30/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6920 - acc: 0.5160\n",
      "Epoch 31/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6918 - acc: 0.5190\n",
      "Epoch 32/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6918 - acc: 0.5170\n",
      "Epoch 33/60\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 0.6917 - acc: 0.5140\n",
      "Epoch 34/60\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 0.6917 - acc: 0.5150\n",
      "Epoch 35/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6915 - acc: 0.5170\n",
      "Epoch 36/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6915 - acc: 0.5220\n",
      "Epoch 37/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6914 - acc: 0.5200\n",
      "Epoch 38/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6913 - acc: 0.5240\n",
      "Epoch 39/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6912 - acc: 0.5210\n",
      "Epoch 40/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6912 - acc: 0.5210\n",
      "Epoch 41/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6911 - acc: 0.5230\n",
      "Epoch 42/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6910 - acc: 0.5260\n",
      "Epoch 43/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6910 - acc: 0.5300\n",
      "Epoch 44/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6909 - acc: 0.5300\n",
      "Epoch 45/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6909 - acc: 0.5320\n",
      "Epoch 46/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6908 - acc: 0.5360\n",
      "Epoch 47/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6908 - acc: 0.5360\n",
      "Epoch 48/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6908 - acc: 0.5350\n",
      "Epoch 49/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6907 - acc: 0.5350\n",
      "Epoch 50/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6906 - acc: 0.5380\n",
      "Epoch 51/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6906 - acc: 0.5390\n",
      "Epoch 52/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6906 - acc: 0.5340\n",
      "Epoch 53/60\n",
      "1000/1000 [==============================] - 0s 10us/step - loss: 0.6906 - acc: 0.5340\n",
      "Epoch 54/60\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6906 - acc: 0.5360\n",
      "Epoch 55/60\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 0.6905 - acc: 0.5380\n",
      "Epoch 56/60\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6905 - acc: 0.5360\n",
      "Epoch 57/60\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6904 - acc: 0.5330\n",
      "Epoch 58/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6904 - acc: 0.5360\n",
      "Epoch 59/60\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6904 - acc: 0.5340\n",
      "Epoch 60/60\n",
      "1000/1000 [==============================] - 0s 11us/step - loss: 0.6904 - acc: 0.5280\n"
     ]
    }
   ],
   "source": [
    "#Use Keras wrapper to package the model as an sklearn object\n",
    "model = KerasClassifier(build_fn=create_dnn_model)\n",
    "# define the grid search parameters\n",
    "batch_size = [32,64,128]\n",
    "epochs = [15, 30, 60]\n",
    "#Create a list with the parameters\n",
    "param_grid =  {\"batch_size\":batch_size, \"epochs\":epochs}\n",
    "#Invoke the grid search method with the list of hyperparameters\n",
    "grid_model = GridSearchCV(estimator=model, \n",
    "                          param_grid=param_grid, \n",
    "                          n_jobs=-1)\n",
    "#Train the model\n",
    "grid_model.fit(x_train, y_train)\n",
    "#Extract the best model grid search\n",
    "best_model = grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model\n",
    "\n",
    "## Saving important weights \n",
    "\n",
    "An example of saving the best weights of a model during training for a \n",
    "large number of epochs is shown in the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 0s 303us/step - loss: 0.7597 - acc: 0.5020\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-06c7c31cde05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m           batch_size=64)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_weight'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    215\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\aims\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m             \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"ModelWeights-{epoch:.2f}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             save_best_only=True, \n",
    "                             monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train,\n",
    "          callbacks=[checkpoint],\n",
    "          epochs=100,\n",
    "          batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save entire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Train a model for defined number of epochs\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=64)\n",
    "\n",
    "# Saves the entire model into a file named as  'dnn_model.h5'\n",
    "model.save('dnn_model.h5')\n",
    "\n",
    "# Later, (maybe another day), you can load the trained model \n",
    "# for prediction.\n",
    "model = load_model('dnn_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (AIMS SA)",
   "language": "python",
   "name": "aims"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
