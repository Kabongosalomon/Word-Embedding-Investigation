{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sentiment Analysis, IMdb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "a5ndLtaz9Fgy",
        "uIaToE3_r2_f",
        "QGQ9aADFPqaz",
        "l_fjg0QZgZCa",
        "7ogYITVq35eG",
        "5LC04oP7ljyP",
        "-3lqU19BPBcx",
        "YryHaIbCPGSj",
        "_C4Trwp-sh_T",
        "YHjkJk0NX-27",
        "4KZ7_dQtoeA0",
        "OM-fJ6Jd8M9S",
        "WJtSfokdJ-7-"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Word-Embedding-Investigation/blob/master/%5BTransformer%2C%20BERT%5D%20An%20empirical%20investigation%20into%20the%20properties%20of%20standard%20word%20embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zPdBDab5P3I"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Kabongosalomon/Data-Science-With-Jeff-Sander/master/aims-za-logo.jpg\" width=\"100\" alt=\"cognitiveclass.ai logo\" />\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v2BR2njWxZFx"
      },
      "source": [
        "# IMdb Sentiment Analysis Task\n",
        "\n",
        "## Importing usefull packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FGvhB20yetpY",
        "outputId": "47f62108-53dc-4a21-c3b5-e2e97e7e5fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "import transformers as ppb\n",
        "\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Downgrate numpy to fix a problem\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "\n",
        "!pip install ipdb\n",
        "import ipdb # deb\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "# Spliting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import metrics # For RUC\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import logging\n",
        "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 9.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 7.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 27.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 35.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 41.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 44.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 47.3MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 50.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 52.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 51.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 53.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 54.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 54.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 54.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 54.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 54.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 54.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 54.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 54.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=9faaa1ddf8ce5563c720f859e955d604fb4971166cbe55bd8cc55870560f25d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/bb/a3e1a441719ebd75c6dac8170d3ddba884b7ee8a5c0f9aefa7297386627a/ipdb-0.13.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (46.1.3)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (1.12.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.2-cp36-none-any.whl size=10522 sha256=c40a408b83d1fef95dd21fc607e60ad9bef39c0d9c970956748a7810db237ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/c2/15/793365e3c9318c46ba914263740d90f1fe67f544b979141ce4\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.13.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkBQByui3Fkt",
        "outputId": "f7d5e5a2-4158-490e-fc62-d5d1399cd1ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "link = \"https://drive.google.com/file/d/1smGRs2g2HoI6VSvonoZmWKzXOP6uPUaW/view?usp=sharing\"\n",
        "\n",
        "_, id_t = link.split('d/')\n",
        "\n",
        "id = id_t.split('/')[0]\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "file_id = id\n",
        "downloaded = drive.CreateFile({'id':file_id})\n",
        "downloaded.FetchMetadata(fetch_all=True)\n",
        "downloaded.GetContentFile(downloaded.metadata['title'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1smGRs2g2HoI6VSvonoZmWKzXOP6uPUaW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JPqYr6Sj3Fk1",
        "outputId": "908b7177-de7d-4eab-e4bd-e7af2af02e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb.zip  adc.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VVDNN97A3Fk5",
        "colab": {}
      },
      "source": [
        "!unzip -qq aclImdb.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YI_7DRX93Fk-",
        "outputId": "6eb077b2-1251-4863-bbb4-e5ea19458b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb  aclImdb.zip  adc.json\t__MACOSX  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lk-Tubsp3FlO",
        "colab": {}
      },
      "source": [
        "# imdb_dir = './data/aclImdb'\n",
        "imdb_dir = './aclImdb'\n",
        "\n",
        "# Reading in the training folder\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "texts_tr_ = []\n",
        "labels_tr = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
        "            texts_tr_.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels_tr.append(0)\n",
        "            else:\n",
        "                labels_tr.append(1)\n",
        "\n",
        "# Reading in the testing folder\n",
        "train_dir = os.path.join(imdb_dir, 'test')\n",
        "texts_tst_ = []\n",
        "labels_tst = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
        "            texts_tst_.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels_tst.append(0)\n",
        "            else:\n",
        "                labels_tst.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "78MzBKaa3FlR",
        "outputId": "65927f29-8847-4898-97ee-fc1945193305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Make sure that we have only 1 and 2 in the label \n",
        "(np.unique(labels_tr), np.unique(labels_tst))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([0, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2-IQncq-pYoD",
        "outputId": "47aefe39-cb1c-47e2-8612-0b4df87d3983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(labels_tr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lt4p5jWS2cXt",
        "outputId": "e4f506de-ffa3-416e-d303-30e9e9d8e2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Looking at 2 examples\n",
        "print(texts_tr_[1])\n",
        "print(labels_tr[1])\n",
        "\n",
        "print(texts_tst_[-10])\n",
        "print(labels_tst[-10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are movies that are awful, and there are movies that are so awful they are deemed long-forgotten and unwatchable. Also, lots of violence and bad stuff (not just cheesy stuff; you know what I mean) add to the mix as well. What is the result of bad movies with such raunchy content? Why, \"Final Justice,\" of course! <br /><br />Remember \"Mitchell?\" Joe Don Baker was the star of that movie, and that was riffed by Joel and the Bots on \"Mystery Science Theater 3000.\" Now this time, with Mike taking Joel's place on the Satellite of Love (but with the same bots), that trio got to make fun of MST3K's second Joe Don Baker movie, \"Final Justice.\" Of course, much of the naughty stuff that I mentioned was removed for television release, but still, I want to watch that episode (and \"Mitchell\" as well), because what does Joe Don \"hate\" the most? Why, none other than \"Mystery Science Theater 3000!\" <br /><br />P.S. If you have a Big Lots nearby, check that store for the uncut tape! LOL That happened to another user!\n",
            "0\n",
            "An assassination thriller in the mould of Day Of The Jackal, In The Line Of Fire has the added twist that Eastwood's old-timer bodyguard Frank Horrigan is troubled by past failure on the job. The chase becomes personal as John Malkovich's reptilian assassin Leary taunts him over this neurosis - with the rather clunky exception of the love of a good woman (Russo's Lilly Raines) nothing's going to set Frank's mind at rest than taking Leary down personally.<br /><br />Malkovich is a volatile presence - not simply combustible on screen but often a changeable actor too. This is one of his good films, focused and playing director Petersen's game. Eastwood is too old but a) the audience don't care, cos it's him and b) his age is written into the script, not only in the narrative but also as a recurring joke. Pretty much as you'd expect but well-handled. 7/10\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nhrYOKaf_t9g"
      },
      "source": [
        "### Getting started\n",
        "\n",
        "#### Data\n",
        "We will try to solve the [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/) task from Mass et al. The dataset consists of IMDB movie reviews labeled by positivity from 1 to 10. The task is to label the reviews as **negative** or **positive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "lVnmgygj_t9i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "15feb242-3f48-4cc0-bf05-beaf1fa544f2"
      },
      "source": [
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "#   data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "#       data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))#; ipdb.set_trace()\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"sentiment\"] = 1\n",
        "  neg_df[\"sentiment\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                       \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "                                      \"aclImdb\", \"test\"))\n",
        "  \n",
        "  return train_df, test_df\n",
        "\n",
        "\n",
        "train_df, test_df = download_and_load_datasets()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 8s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CR4TW4SzGerx",
        "outputId": "9ec1c802-c7bb-4818-810f-4a25e3138d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Double-check that we've done the right thing.\n",
        "print(\"Training examples summary:\")\n",
        "display.display(train_df.describe())\n",
        "display.display(train_df.head())\n",
        "print(\"\\n#############################\\n\")\n",
        "print(\"Validation examples summary:\")\n",
        "display.display(test_df.describe())\n",
        "display.display(test_df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>25000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment\n",
              "count  25000.00000\n",
              "mean       0.50000\n",
              "std        0.50001\n",
              "min        0.00000\n",
              "25%        0.00000\n",
              "50%        0.50000\n",
              "75%        1.00000\n",
              "max        1.00000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Personnaly I really loved this movie, and it p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This movie made me very happy. It's impossible...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film is the worst film I have ever seen. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Like the gentle giants that make up the latter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Things get dull early an often in this in this...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  sentiment\n",
              "0  Personnaly I really loved this movie, and it p...          1\n",
              "1  This movie made me very happy. It's impossible...          1\n",
              "2  This film is the worst film I have ever seen. ...          0\n",
              "3  Like the gentle giants that make up the latter...          1\n",
              "4  Things get dull early an often in this in this...          0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#############################\n",
            "\n",
            "Validation examples summary:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>25000.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.50001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment\n",
              "count  25000.00000\n",
              "mean       0.50000\n",
              "std        0.50001\n",
              "min        0.00000\n",
              "25%        0.00000\n",
              "50%        0.50000\n",
              "75%        1.00000\n",
              "max        1.00000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Burt Reynolds' riposte to Clint Eastwood encro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a film by Oshima, the director of the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Such a shame that this wonderful bright spot o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>If I had never seen the first Road House, then...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This movie tells the story of nine ambitious t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  sentiment\n",
              "0  Burt Reynolds' riposte to Clint Eastwood encro...          1\n",
              "1  This is a film by Oshima, the director of the ...          1\n",
              "2  Such a shame that this wonderful bright spot o...          1\n",
              "3  If I had never seen the first Road House, then...          0\n",
              "4  This movie tells the story of nine ambitious t...          1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYwB0uArqmex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "18b68291-1c05-4998-dce8-d3f0ded961b2"
      },
      "source": [
        "train_df['sentence'].apply(lambda x : len(x))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         774\n",
              "1         300\n",
              "2         383\n",
              "3        1126\n",
              "4        2586\n",
              "         ... \n",
              "24995    3615\n",
              "24996     578\n",
              "24997     730\n",
              "24998     411\n",
              "24999     951\n",
              "Name: sentence, Length: 25000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO58JLHOpzhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa1356ab-0d69-4aff-cff1-a0420e23552f"
      },
      "source": [
        "# mean lengh of review \n",
        "train_df['sentence'].apply(lambda x : len(x)).mean()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1325.06964"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6iNailrqXZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "09405f36-d8bd-4fa4-9524-e9d1b2e312d5"
      },
      "source": [
        "train_df['sentence'][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Personnaly I really loved this movie, and it particularly moved me. The two main actors are giving us such great performances, that at the end, it is really heart breaking to know what finally happened to their characters.<br /><br />The alchemy between Barbra Streisand and Kris Kristofferson is marvelous, and the song are just great the way they are. <br /><br />That\\'s why I didn\\'t feel surprised when I learned it had won 5 golden globe awards (the most rewarded movie at the Golden Globes), an Oscar and even a Grammy. This movie is a classic that deserves to be seen by anyone. A great movie, that has often been criticized (maybe because Streisand dared to get involved in it, surely as a \"co-director\"). Her artistry is the biggest, and that will surely please you!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGlGdM4vJ2uc",
        "colab_type": "text"
      },
      "source": [
        "# Obtain Pre-trained BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASGXbiRKJ2uc",
        "colab_type": "code",
        "outputId": "b08d28ae-9462-464d-e8a1-164c87fe56d5",
        "colab": {}
      },
      "source": [
        "# !pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 158kB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (4.37.0)\n",
            "Requirement already satisfied: sentencepiece in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (0.1.85)\n",
            "Requirement already satisfied: requests in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (2.22.0)\n",
            "Requirement already satisfied: torch in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (1.3.0)\n",
            "Requirement already satisfied: six in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied: numpy in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from torchtext) (1.16.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->torchtext) (1.25.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->torchtext) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/aims/anaconda3/envs/aims/lib/python3.7/site-packages (from requests->torchtext) (2.8)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnwOOnVcJ2ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # define VGG16 model\n",
        "# # VGG16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# DistilBERT = models.transformers()\n",
        "\n",
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "# move model to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "    # tokenizer = tokenizer.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxAMrw7DJ2uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LossPrettifier(object):\n",
        "    \n",
        "    STYLE = {\n",
        "        'green' : '\\033[32m',\n",
        "        'red'   : '\\033[91m', \n",
        "        'bold'  : '\\033[1m', \n",
        "    }\n",
        "    STYLE_END = '\\033[0m'\n",
        "    \n",
        "    def __init__(self, show_percentage=False):\n",
        "        \n",
        "        self.show_percentage = show_percentage\n",
        "        self.color_up = 'green'\n",
        "        self.color_down = 'red'\n",
        "        self.loss_terms = {}\n",
        "    \n",
        "    def __call__(self, epoch=None, **kwargs):\n",
        "        \n",
        "        if epoch is not None:\n",
        "            print_string = f'Epoch {epoch: 5d} '\n",
        "        else:\n",
        "            print_string = ''\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            \n",
        "            pre_value = self.loss_terms.get(key, value)\n",
        "            \n",
        "            if value > pre_value:\n",
        "                indicator  = '▲'\n",
        "                show_color = self.STYLE[self.color_up]\n",
        "            elif value == pre_value:\n",
        "                indicator  = ''\n",
        "                show_color = ''\n",
        "            else:\n",
        "                indicator  = '▼'\n",
        "                show_color = self.STYLE[self.color_down]\n",
        "            \n",
        "            if self.show_percentage:\n",
        "                show_value = 0 if pre_value == 0 \\\n",
        "                             else (value - pre_value) / float(pre_value)\n",
        "                key_string = f'| {key}: {show_color}{value:3.4f}({show_value:+3.4%}) {indicator}'\n",
        "            else: \n",
        "                key_string = f'| {key}: {show_color}{value:.4f} {indicator}'\n",
        "            \n",
        "            # Trim some long outputs\n",
        "            key_string_part = key_string[:32]\n",
        "            print_string += key_string_part+f'{self.STYLE_END}\\t'\n",
        "            \n",
        "            self.loss_terms[key] = value\n",
        "            \n",
        "        print(print_string)\n",
        "reporter = LossPrettifier(show_percentage=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugm_F1OSJ2ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss)) \n",
        "            \n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## update the average validation loss\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # update average validation loss \n",
        "            valid_loss +=((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "            \n",
        "            \n",
        "        # print training/validation statistics \n",
        "        \n",
        "        reporter(epoch=epoch, LossA = train_loss, LossB = valid_loss)\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            valid_loss_min = valid_loss\n",
        "            \n",
        "    # return trained model\n",
        "    return model\n",
        "\n",
        "# loaders_scratch = {'train' : train_loader, 'valid': valid_loader, 'test' :test_loader}\n",
        "\n",
        "# # train the model\n",
        "# model_scratch = train(15, loaders_scratch, model_scratch, optimizer_scratch, \n",
        "#                       criterion_scratch, use_cuda, 'model_scratch.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itwiwBbYJ2ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "# test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOL_1n3ghhV",
        "colab_type": "code",
        "outputId": "f6764b68-b313-459c-e369-175fff82ee12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34maclImdb\u001b[0m/  aclImdb.zip  adc.json  \u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDBMn3wiSX6",
        "colab_type": "text"
      },
      "source": [
        "Right now, the variable `model` holds a pretrained distilBERT model -- a version of BERT that is smaller, but much faster and requiring a lot less memory.\n",
        "\n",
        "## Model #1: Preparing the Dataset\n",
        "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
        "\n",
        "### Tokenization\n",
        "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with.\n",
        "\n",
        "Unfortunatly the bert tokenizer can only encode sequence lenght less or equal to 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_train = train_df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512))) \n",
        "tokenized_test = test_df['sentence'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwjUwYgi-uL",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
        "\n",
        "### Padding\n",
        "After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized_train.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
        "\n",
        "# max_len = 0\n",
        "# for i in tokenized_train.values:\n",
        "#     if len(i) > max_len:\n",
        "#         max_len = len(i)\n",
        "\n",
        "# padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdjg306wjjmL",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is now in the `padded` variable, we can view its dimensions below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi7uXo95xeq",
        "colab_type": "code",
        "outputId": "1e1f87be-adea-480c-991c-5a455e173075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDZBsYSDjzDV",
        "colab_type": "text"
      },
      "source": [
        "### Masking\n",
        "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_iGRNa_Ozc",
        "colab_type": "code",
        "outputId": "db2aedba-ec7e-4b06-97b8-b6df0f90a1df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-CQB9-kN99",
        "colab_type": "text"
      },
      "source": [
        "## Model #1: And Now, Deep Learning!\n",
        "Now that we have our model and inputs ready, let's run our model!\n",
        "\n",
        "<img src=\"http://127.0.0.1:4000/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n",
        "\n",
        "The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UVjAV56PJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "a764debc-fe7f-4cd2-bf0a-846708c7b5a7"
      },
      "source": [
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-592737a6eacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mtfmr_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfmr_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoCep_WVuB3v",
        "colab_type": "text"
      },
      "source": [
        "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
        "\n",
        "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
        "\n",
        "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9t60At16PVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VZVU66Gurr-",
        "colab_type": "text"
      },
      "source": [
        "The labels indicating which sentence is positive and negative now go into the `labels` variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3fX2yh6PTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = batch_1[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoEvM2evRx1",
        "colab_type": "text"
      },
      "source": [
        "## Model #2: Train/Test Split\n",
        "Let's now split our datset into a training set and testing set (even though we're using 2,000 sentences from the SST2 training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Kw-MIwJ2uc",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
        "\n",
        "Under the hood, the model is actually made up of two model.\n",
        "\n",
        "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
        "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
        "\n",
        "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYM9oAkLJ2uk",
        "colab_type": "code",
        "outputId": "603afded-4758-40ce-ba40-1f16427658de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "## TODO: Specify data loaders\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "\n",
        "# define training, valid and test data directories\n",
        "data_dir = './aclImdb/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "valid_dir = os.path.join(data_dir, 'valid/')\n",
        "test_dir = os.path.join(data_dir, 'test/')\n",
        "\n",
        "# data_transform = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(224),\n",
        "#     transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "# #     transforms.RandomRotation(10),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#     ])\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir)\n",
        "# valid_data = datasets.ImageFolder(valid_dir)\n",
        "test_data = datasets.ImageFolder(test_dir)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    num_workers=num_workers)\n",
        "# valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "#     num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "    num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7b3b332f3ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     ])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m# valid_data = datasets.ImageFolder(valid_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n\u001b[0;32m---> 97\u001b[0;31m                                 \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./aclImdb/train/\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo-RpErIJ2um",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Model Architecture\n",
        "\n",
        "Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y8ZoLH3J2un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "## TODO: Specify model architecture \n",
        "# Load the pretrained model from pytorch\n",
        "model_transfer = models.vgg16(pretrained=True)\n",
        "\n",
        "# Freeze training for all \"features\" layers\n",
        "for param in model_transfer.features.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "import torch.nn as nn\n",
        "\n",
        "n_inputs = model_transfer.classifier[6].in_features\n",
        "\n",
        "# add last linear layer (n_inputs -> 5 flower classes)\n",
        "# new layers automatically have requires_grad = True\n",
        "last_layer = nn.Linear(n_inputs, 133)\n",
        "\n",
        "model_transfer.classifier[6] = last_layer\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(model_transfer.classifier[6].out_features)\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer = model_transfer.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QmbTyZJ2uo",
        "colab_type": "text"
      },
      "source": [
        "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eaf8jJO_J2uo",
        "colab_type": "text"
      },
      "source": [
        "__Answer:__ \n",
        "\n",
        "- I used the pretrained vgg16 and just changed the last layer to output 133 classes.\n",
        "- The reason behind using the vgg16 is the it's a pre-trained model on huge images and by freezing the weight we can take advantage of what the model learned from the original training data. \n",
        "- I thin kthis architecture is suitable for the dog breed problem because the vgg16 also was trained on imagenet and some of those have some caracteristic in common with my task. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPCGT9nRJ2up",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
        "\n",
        "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS95TPR6J2up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "optimizer_transfer = optim.SGD(model_transfer.classifier.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwUFvwVcJ2uq",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Train and Validate the Model\n",
        "\n",
        "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at filepath `'model_transfer.pt'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vqrZB7nJ2uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders_transfer = {'train' : train_loader, 'valid': valid_loader, 'test' :test_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SlgXZYLJ2ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "model_transfer = train(15, loaders_transfer, model_transfer, \n",
        "                       optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8eKaokMJ2uv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model that got the best validation accuracy (uncomment the line below)\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUU5VUBhJ2uy",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Test the Model\n",
        "\n",
        "Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NegpbjoiJ2uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Yp3RBRJ2u0",
        "colab_type": "text"
      },
      "source": [
        "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
        "\n",
        "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan hound`, etc) that is predicted by your model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxTMZlp_J2u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/data/dog_images/train/001.Affenpinscher/Affenpinscher_00001.jpg'\n",
        "data_transfer = {'train' : train_data, 'valid': valid_data, 'test' :test_data}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hHtZtFFJ2u3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TODO: Write a function that takes a path to an image as input\n",
        "### and returns the dog breed that is predicted by the model.\n",
        "\n",
        "# list of class names by index, i.e. a name can be accessed like class_names[0]\n",
        "class_names = [item[4:].replace(\"_\", \" \") for item in data_transfer['train'].classes]\n",
        "\n",
        "def predict_breed_transfer(img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    \n",
        "    data_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    \n",
        "    img = data_transform(Image.open(img_path))\n",
        "    if use_cuda:\n",
        "        img = img.cuda()\n",
        "        \n",
        "    # PyTorch pretrained models expect the Tensor dims to be (num input imgs, num color channels, height, width).\n",
        "    # Currently however, we have (num color channels, height, width); let's fix this by inserting a new axis.\n",
        "    img = img.unsqueeze(0)  # Insert the new axis at index 0 i.e. in front of the other axes/dims. \n",
        "    \n",
        "    prediction = model_transfer(img)  # Returns a Tensor of shape (batch, num class labels)\n",
        "\n",
        "    \n",
        "    return class_names[prediction.data.cpu().numpy().argmax()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nY-nPjpa4wDr"
      },
      "source": [
        "---\n",
        "\n",
        "# <h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://salomonkabongo.wixsite.com/datascientist\">Salomon Kabongo KABENAMUALU</a>, Master degree student at <a href=\"https://aims.ac.za/\">the African Institute for mathematical SCiences (AIMS South Africa)</a> his research focused on the use machine learning technique in the field of Natural Language Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xDX0h3pa4wDt"
      },
      "source": [
        "References : <a href=\"https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/\"> How to Develop a Deep Learning Bag-of-Words Model for Predicting Movie Review Sentiment</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RdkZJcdh4wDu"
      },
      "source": [
        "Copyright &copy; 2019. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
      ]
    }
  ]
}